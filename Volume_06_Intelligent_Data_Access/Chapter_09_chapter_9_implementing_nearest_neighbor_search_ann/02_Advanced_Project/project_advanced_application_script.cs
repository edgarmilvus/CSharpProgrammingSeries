
#
# These sources are part of the "C# Programming Series" by Edgar Milvus, 
# you can find it on stores: 
# 
# https://www.amazon.com/dp/B0GKJ3NYL6 or https://tinyurl.com/CSharpProgrammingBooks or 
# https://leanpub.com/u/edgarmilvus (quantity discounts)
# 
# New books info: https://linktr.ee/edgarmilvus 
#
# MIT License
# Copyright (c) 2026 Edgar Milvus
# Permission is hereby granted, free of charge, to any person obtaining a copy
# of this software and associated documentation files (the "Software"), to deal
# in the Software without restriction, including without limitation the rights
# to use, copy, modify, merge, publish, distribute, sublicense, and/or sell
# copies of the Software, and to permit persons to whom the Software is
# furnished to do so, subject to the following conditions:
#
# The above copyright notice and this permission notice shall be included in all
# copies or substantial portions of the Software.
#
# THE SOFTWARE IS PROVIDED "AS IS", WITHOUT WARRANTY OF ANY KIND, EXPRESS OR
# IMPLIED, INCLUDING BUT NOT LIMITED TO THE WARRANTIES OF MERCHANTABILITY,
# FITNESS FOR A PARTICULAR PURPOSE AND NONINFRINGEMENT. IN NO EVENT SHALL THE
# AUTHORS OR COPYRIGHT HOLDERS BE LIABLE FOR ANY CLAIM, DAMAGES OR OTHER
# LIABILITY, WHETHER IN AN ACTION OF CONTRACT, TORT OR OTHERWISE, ARISING FROM,
# OUT OF OR IN CONNECTION WITH THE SOFTWARE OR THE USE OR OTHER DEALINGS IN THE
# SOFTWARE.

# Source File: project_advanced_application_script.cs
# Description: Advanced Application Script
# ==========================================

using System;
using System.Collections.Generic;
using System.IO;
using System.Text;

namespace VectorSearchRAG
{
    // Real-world context: A document retrieval system for a corporate knowledge base.
    // Employees ask questions in natural language. We convert these questions into
    // numerical "embeddings" (vectors) and search for the most relevant document chunks
    // using Approximate Nearest Neighbor (ANN) search to provide context for an LLM.
    class Program
    {
        static void Main(string[] args)
        {
            Console.WriteLine("=== Corporate Knowledge Base RAG System ===\n");

            // 1. Initialize Data Store (Simulating a Vector Database)
            // In a real scenario, this would be PostgreSQL with pgvector or a dedicated vector DB.
            // We store text alongside their vector representations.
            List<DocumentChunk> knowledgeBase = new List<DocumentChunk>();

            // 2. Populate with Dummy Data (Embeddings usually generated by AI models)
            // We use simple 3D vectors for demonstration. Real embeddings are 1536+ dimensions.
            // Topic: Server Maintenance Procedures
            knowledgeBase.Add(new DocumentChunk
            {
                Id = 1,
                Text = "To restart the production server, use the command: sudo systemctl restart nginx.",
                Vector = new float[] { 0.1f, 0.2f, 0.9f } // High relevance to "restart" and "nginx"
            });

            knowledgeBase.Add(new DocumentChunk
            {
                Id = 2,
                Text = "Check disk usage using 'df -h' to ensure sufficient storage space.",
                Vector = new float[] { 0.8f, 0.1f, 0.1f } // High relevance to "disk" or "storage"
            });

            knowledgeBase.Add(new DocumentChunk
            {
                Id = 3,
                Text = "To restart the nginx service on the staging server, use: sudo systemctl restart nginx.",
                Vector = new float[] { 0.15f, 0.25f, 0.85f } // Similar to ID 1, but "staging"
            });

            knowledgeBase.Add(new DocumentChunk
            {
                Id = 4,
                Text = "Memory usage can be monitored via 'htop' or 'free -m' commands.",
                Vector = new float[] { 0.75f, 0.15f, 0.2f } // Related to performance metrics
            });

            // 3. User Query Processing
            string userQuery = "How do I restart the server?";
            Console.WriteLine($"User Query: \"{userQuery}\"");

            // Simulate embedding generation for the query
            // In reality, this calls an AI model (e.g., OpenAI Ada-002).
            // We manually assign a vector that mathematically relates to the target documents.
            float[] queryVector = new float[] { 0.12f, 0.22f, 0.88f };

            // 4. Execute Approximate Nearest Neighbor (ANN) Search
            // We implement a basic Linear Scan (Brute Force) ANN for simplicity,
            // which is accurate but slower than HNSW for massive datasets.
            List<SearchResult> searchResults = PerformLinearScanANN(knowledgeBase, queryVector, topK: 2);

            // 5. Construct RAG (Retrieval-Augmented Generation) Prompt
            Console.WriteLine("\n--- Retrieval Phase (ANN Search Results) ---");
            StringBuilder contextBuilder = new StringBuilder();
            
            foreach (var result in searchResults)
            {
                Console.WriteLine($"[Score: {result.Similarity:F4}] ID: {result.Chunk.Id} - {result.Chunk.Text}");
                contextBuilder.AppendLine(result.Chunk.Text);
            }

            // 6. Simulate LLM Generation
            Console.WriteLine("\n--- Generation Phase (LLM Response) ---");
            string ragPrompt = $"Context:\n{contextBuilder.ToString()}\n\nQuestion: {userQuery}\n\nAnswer:";
            string generatedResponse = SimulateLLM(ragPrompt);
            Console.WriteLine(generatedResponse);
        }

        // Core ANN Logic: Cosine Similarity Calculation
        // Calculates the cosine of the angle between two vectors.
        // Range: -1 (opposite) to 1 (identical). 1 is perfect match.
        static float CalculateCosineSimilarity(float[] vecA, float[] vecB)
        {
            if (vecA.Length != vecB.Length)
                throw new ArgumentException("Vector dimensions must match.");

            float dotProduct = 0.0f;
            float magnitudeA = 0.0f;
            float magnitudeB = 0.0f;

            // Basic loop for dot product and magnitude calculation
            for (int i = 0; i < vecA.Length; i++)
            {
                dotProduct += vecA[i] * vecB[i];
                magnitudeA += vecA[i] * vecA[i];
                magnitudeB += vecB[i] * vecB[i];
            }

            magnitudeA = (float)Math.Sqrt(magnitudeA);
            magnitudeB = (float)Math.Sqrt(magnitudeB);

            // Avoid division by zero
            if (magnitudeA == 0 || magnitudeB == 0) return 0;

            return dotProduct / (magnitudeA * magnitudeB);
        }

        // ANN Search Implementation: Linear Scan
        // Iterates through all items (O(N) complexity). 
        // For production HNSW, we would use a library like PgVector or Milvus.
        static List<SearchResult> PerformLinearScanANN(List<DocumentChunk> database, float[] queryVector, int topK)
        {
            List<SearchResult> allScores = new List<SearchResult>();

            // Step 1: Calculate similarity for every document in the database
            foreach (var doc in database)
            {
                float score = CalculateCosineSimilarity(doc.Vector, queryVector);
                allScores.Add(new SearchResult { Chunk = doc, Similarity = score });
            }

            // Step 2: Sort results to find the "Nearest Neighbors"
            // Using simple bubble sort for educational clarity (avoiding LINQ/Array.Sort)
            for (int i = 0; i < allScores.Count - 1; i++)
            {
                for (int j = 0; j < allScores.Count - i - 1; j++)
                {
                    // Sort in Descending order (highest similarity first)
                    if (allScores[j].Similarity < allScores[j + 1].Similarity)
                    {
                        // Swap
                        SearchResult temp = allScores[j];
                        allScores[j] = allScores[j + 1];
                        allScores[j + 1] = temp;
                    }
                }
            }

            // Step 3: Return top K results
            List<SearchResult> topResults = new List<SearchResult>();
            for (int i = 0; i < Math.Min(topK, allScores.Count); i++)
            {
                topResults.Add(allScores[i]);
            }

            return topResults;
        }

        // Simulates an LLM call (e.g., GPT-4) taking the context and question to generate an answer.
        static string SimulateLLM(string prompt)
        {
            // In a real app, this would be an HTTP request to an AI endpoint.
            // Here, we simply parse the context to mock a smart response.
            if (prompt.Contains("restart") && prompt.Contains("nginx"))
            {
                return "LLM: Based on the documentation, you can restart the server by running: 'sudo systemctl restart nginx'. Ensure you have root access.";
            }
            return "LLM: I am unsure based on the provided context. Please consult a system administrator.";
        }
    }

    // Data Models
    public class DocumentChunk
    {
        public int Id { get; set; }
        public string Text { get; set; }
        public float[] Vector { get; set; } // The high-dimensional embedding
    }

    public class SearchResult
    {
        public DocumentChunk Chunk { get; set; }
        public float Similarity { get; set; } // Cosine similarity score
    }
}
