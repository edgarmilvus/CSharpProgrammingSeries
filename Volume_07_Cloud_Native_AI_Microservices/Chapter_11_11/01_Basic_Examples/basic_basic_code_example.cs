
#
# These sources are part of the "C# Programming Series" by Edgar Milvus, 
# you can find it on stores: 
# 
# https://www.amazon.com/dp/B0GKJ3NYL6 or https://tinyurl.com/CSharpProgrammingBooks or 
# https://leanpub.com/u/edgarmilvus (quantity discounts)
# 
# New books info: https://linktr.ee/edgarmilvus 
#
# MIT License
# Copyright (c) 2026 Edgar Milvus
# Permission is hereby granted, free of charge, to any person obtaining a copy
# of this software and associated documentation files (the "Software"), to deal
# in the Software without restriction, including without limitation the rights
# to use, copy, modify, merge, publish, distribute, sublicense, and/or sell
# copies of the Software, and to permit persons to whom the Software is
# furnished to do so, subject to the following conditions:
#
# The above copyright notice and this permission notice shall be included in all
# copies or substantial portions of the Software.
#
# THE SOFTWARE IS PROVIDED "AS IS", WITHOUT WARRANTY OF ANY KIND, EXPRESS OR
# IMPLIED, INCLUDING BUT NOT LIMITED TO THE WARRANTIES OF MERCHANTABILITY,
# FITNESS FOR A PARTICULAR PURPOSE AND NONINFRINGEMENT. IN NO EVENT SHALL THE
# AUTHORS OR COPYRIGHT HOLDERS BE LIABLE FOR ANY CLAIM, DAMAGES OR OTHER
# LIABILITY, WHETHER IN AN ACTION OF CONTRACT, TORT OR OTHERWISE, ARISING FROM,
# OUT OF OR IN CONNECTION WITH THE SOFTWARE OR THE USE OR OTHER DEALINGS IN THE
# SOFTWARE.

# Source File: basic_basic_code_example.cs
# Description: Basic Code Example
# ==========================================

using System;
using System.Collections.Generic;
using System.Linq;
using System.Text.Json;
using System.Threading.Tasks;
using Microsoft.Extensions.Hosting;
using Microsoft.Extensions.DependencyInjection;
using Microsoft.Extensions.Logging;
using Microsoft.Extensions.Options;
using Microsoft.Extensions.Configuration;
using Microsoft.Extensions.Configuration.Json;
using System.IO;
using System.Text.Json.Serialization;

namespace CloudNativeAI.Microservices
{
    // ==================== CORE DOMAIN MODELS ====================
    // These models represent the data contracts for our AI inference service.
    // In a production environment, these would likely be defined in a shared library
    // or generated via gRPC/Protobuf for strict schema enforcement.

    /// <summary>
    /// Represents an incoming inference request from a client.
    /// In a real-world scenario, this might be a user prompt, an image tensor,
    /// or a batch of data points.
    /// </summary>
    public record InferenceRequest
    {
        [JsonPropertyName("prompt")]
        public string Prompt { get; init; } = string.Empty;

        [JsonPropertyName("request_id")]
        public string RequestId { get; init; } = Guid.NewGuid().ToString();

        [JsonPropertyName("timestamp")]
        public DateTime Timestamp { get; init; } = DateTime.UtcNow;

        [JsonPropertyName("parameters")]
        public Dictionary<string, object>? Parameters { get; init; }
    }

    /// <summary>
    /// Represents the response generated by the AI model.
    /// </summary>
    public record InferenceResponse
    {
        [JsonPropertyName("result")]
        public string Result { get; init; } = string.Empty;

        [JsonPropertyName("request_id")]
        public string RequestId { get; init; } = string.Empty;

        [JsonPropertyName("processing_time_ms")]
        public long ProcessingTimeMs { get; init; }

        [JsonPropertyName("model_version")]
        public string ModelVersion { get; init; } = "v1.0";
    }

    // ==================== ABSTRACTIONS ====================

    /// <summary>
    /// Defines the contract for an AI model executor.
    /// This abstraction allows us to swap out different model backends
    /// (e.g., ONNX Runtime, TensorFlow.NET, or a remote HTTP API) without changing the service logic.
    /// </summary>
    public interface IModelExecutor
    {
        Task<InferenceResponse> ExecuteAsync(InferenceRequest request);
    }

    // ==================== CONCRETE IMPLEMENTATIONS ====================

    /// <summary>
    /// A mock implementation of an AI model executor.
    /// In a real containerized environment, this would interface with a loaded model file
    /// (e.g., a .onnx file) and a runtime engine.
    /// </summary>
    public class MockTransformerModelExecutor : IModelExecutor
    {
        private readonly ILogger<MockTransformerModelExecutor> _logger;
        private readonly ModelConfig _config;
        private bool _isModelLoaded = false;

        public MockTransformerModelExecutor(ILogger<MockTransformerModelExecutor> logger, IOptions<ModelConfig> config)
        {
            _logger = logger;
            _config = config.Value;
        }

        public async Task<InferenceResponse> ExecuteAsync(InferenceRequest request)
        {
            EnsureModelLoaded();

            // Simulate the latency of model inference.
            // In a real GPU-bound workload, this delay represents the time to transfer
            // data to VRAM, execute kernels, and retrieve results.
            var stopwatch = System.Diagnostics.Stopwatch.StartNew();
            
            _logger.LogInformation("Processing request {RequestId} with prompt: {Prompt}", 
                request.RequestId, request.Prompt);

            // Simulate "thinking" time based on prompt length (heuristic for demo)
            await Task.Delay(Math.Min(2000, request.Prompt.Length * 10)); 

            stopwatch.Stop();

            // Simulate a simple generative response
            string result = $"Generated response for: '{request.Prompt}' (Model: {_config.Name}, Version: {_config.Version})";

            _logger.LogInformation("Completed request {RequestId} in {Elapsed}ms", 
                request.RequestId, stopwatch.ElapsedMilliseconds);

            return new InferenceResponse
            {
                Result = result,
                RequestId = request.RequestId,
                ProcessingTimeMs = stopwatch.ElapsedMilliseconds,
                ModelVersion = _config.Version
            };
        }

        private void EnsureModelLoaded()
        {
            if (!_isModelLoaded)
            {
                _logger.LogInformation("Loading model '{ModelName}' into memory...", _config.Name);
                // Simulate I/O bound model loading (reading from disk/network)
                Thread.Sleep(500); 
                _isModelLoaded = true;
                _logger.LogInformation("Model '{ModelName}' loaded successfully.", _config.Name);
            }
        }
    }

    // ==================== CONFIGURATION ====================

    public class ModelConfig
    {
        public string Name { get; set; } = "DefaultModel";
        public string Version { get; set; } = "1.0.0";
        public int MaxBatchSize { get; set; } = 32;
    }

    public class ServiceConfig
    {
        public int Port { get; set; } = 8080;
    }

    // ==================== HTTP API LAYER ====================

    /// <summary>
    /// A minimal HTTP API endpoint handler.
    /// In a production setting, this would be an ASP.NET Core Controller or Minimal API endpoint.
    /// </summary>
    public class InferenceApiHandler
    {
        private readonly IModelExecutor _modelExecutor;
        private readonly ILogger<InferenceApiHandler> _logger;

        public InferenceApiHandler(IModelExecutor modelExecutor, ILogger<InferenceApiHandler> logger)
        {
            _modelExecutor = modelExecutor;
            _logger = logger;
        }

        public async Task HandleRequestAsync(HttpListenerContext context)
        {
            try
            {
                if (context.Request.HttpMethod != "POST" || !context.Request.Url.AbsolutePath.Equals("/infer"))
                {
                    context.Response.StatusCode = 404;
                    await context.Response.OutputStream.WriteAsync(System.Text.Encoding.UTF8.GetBytes("Not Found"));
                    return;
                }

                using var reader = new StreamReader(context.Request.InputStream);
                var json = await reader.ReadToEndAsync();
                var request = JsonSerializer.Deserialize<InferenceRequest>(json);

                if (request == null || string.IsNullOrWhiteSpace(request.Prompt))
                {
                    context.Response.StatusCode = 400;
                    await context.Response.OutputStream.WriteAsync(System.Text.Encoding.UTF8.GetBytes("Invalid Request: Prompt is required."));
                    return;
                }

                var response = await _modelExecutor.ExecuteAsync(request);
                
                var jsonResponse = JsonSerializer.Serialize(response, new JsonSerializerOptions { WriteIndented = true });
                
                context.Response.ContentType = "application/json";
                context.Response.StatusCode = 200;
                var buffer = System.Text.Encoding.UTF8.GetBytes(jsonResponse);
                await context.Response.OutputStream.WriteAsync(buffer);
            }
            catch (Exception ex)
            {
                _logger.LogError(ex, "Error handling inference request");
                context.Response.StatusCode = 500;
                await context.Response.OutputStream.WriteAsync(System.Text.Encoding.UTF8.GetBytes($"Internal Server Error: {ex.Message}"));
            }
            finally
            {
                context.Response.Close();
            }
        }
    }

    // ==================== HOSTING INFRASTRUCTURE ====================

    /// <summary>
    /// Background service that listens for HTTP requests and delegates to the handler.
    /// This mimics the behavior of a web server running inside a container.
    /// </summary>
    public class InferenceHostedService : IHostedService
    {
        private readonly HttpListener _listener;
        private readonly InferenceApiHandler _handler;
        private readonly ILogger<InferenceHostedService> _logger;
        private readonly ServiceConfig _config;
        private Task? _listeningTask;
        private CancellationTokenSource? _cts;

        public InferenceHostedService(InferenceApiHandler handler, ILogger<InferenceHostedService> logger, IOptions<ServiceConfig> config)
        {
            _handler = handler;
            _logger = logger;
            _config = config.Value;
            _listener = new HttpListener();
            // Note: HttpListener requires URL ACL setup (netsh) or running as admin on Windows.
            // For Linux/macOS, prefix usually requires sudo or specific capabilities.
            // For this example, we use localhost.
            _listener.Prefixes.Add($"http://localhost:{_config.Port}/");
        }

        public async Task StartAsync(CancellationToken cancellationToken)
        {
            _cts = CancellationTokenSource.CreateLinkedTokenSource(cancellationToken);
            _listener.Start();
            _logger.LogInformation("Inference Service started on port {Port}", _config.Port);

            _listeningTask = Task.Run(async () =>
            {
                while (!_cts.Token.IsCancellationRequested)
                {
                    try
                    {
                        // Asynchronously wait for an incoming connection
                        var context = await _listener.GetContextAsync();
                        
                        // Handle request in a fire-and-forget manner (or use a limited concurrency queue)
                        // For production, use a SemaphoreSlim or Channels to limit concurrent requests
                        // to prevent OOM on the container.
                        _ = Task.Run(() => _handler.HandleRequestAsync(context), _cts.Token);
                    }
                    catch (HttpListenerException) when (_cts.Token.IsCancellationRequested)
                    {
                        // Expected when stopping
                        break;
                    }
                    catch (Exception ex)
                    {
                        _logger.LogError(ex, "Error accepting connection");
                    }
                }
            }, _cts.Token);
        }

        public async Task StopAsync(CancellationToken cancellationToken)
        {
            _logger.LogInformation("Stopping Inference Service...");
            _cts?.Cancel();
            _listener.Stop();
            _listener.Close();
            
            if (_listeningTask != null)
                await _listeningTask;
        }
    }

    // ==================== MAIN PROGRAM ENTRY ====================

    public class Program
    {
        public static async Task Main(string[] args)
        {
            // Configure the host with Dependency Injection
            var host = Host.CreateDefaultBuilder(args)
                .ConfigureAppConfiguration((context, config) =>
                {
                    // In a container, we might mount a ConfigMap as a JSON file
                    config.AddJsonFile("appsettings.json", optional: true, reloadOnChange: true);
                })
                .ConfigureServices((context, services) =>
                {
                    // Bind configuration sections
                    services.Configure<ModelConfig>(context.Configuration.GetSection("Model"));
                    services.Configure<ServiceConfig>(context.Configuration.GetSection("Service"));

                    // Register dependencies
                    services.AddSingleton<IModelExecutor, MockTransformerModelExecutor>();
                    services.AddSingleton<InferenceApiHandler>();
                    
                    // Register the hosted service (the actual server)
                    services.AddHostedService<InferenceHostedService>();
                })
                .ConfigureLogging(logging =>
                {
                    logging.ClearProviders();
                    logging.AddConsole();
                    logging.SetMinimumLevel(LogLevel.Information);
                })
                .Build();

            // Run the host
            await host.RunAsync();
        }
    }
}
